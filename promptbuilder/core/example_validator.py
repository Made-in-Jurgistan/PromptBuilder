"""
Example Validator Module for PromptBuilder.

This module provides validation functionality for examples generated by the
ExampleGenerator. It performs comprehensive validation including code syntax checking,
technology compatibility verification, and content quality assessment.

Key features:
  - Structural validation of examples
  - Code syntax and security validation
  - Technology compatibility checking
  - Content quality assessment
  - Detailed validation metrics collection
  - Language-specific validation rules

Usage:
    from promptbuilder.core.example_validator import ExampleValidator
    validator = ExampleValidator()
    valid_examples, metrics = validator.validate_examples(examples)

Author: Made in Jurgistan
Version: 2.1.0
License: MIT
"""

import ast
import logging
import time
import re
import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any, Union, Callable, cast
from collections import defaultdict
import uuid

from promptbuilder.core.query_components import (
    QueryComponents,
    QueryInfo,
    Example,
    QueryType,
    DifficultyLevel,
    FrameworkType
)


class ExampleValidator:
    """Validates examples generated by the ExampleGenerator.
    
    This class provides functionality for validating examples at multiple levels, 
    from simple structural validation to complex code syntax checking and 
    content quality assessment. It collects detailed metrics about the validation
    process and results.
    
    Attributes:
        logger: Logger instance for tracking validation operations
        supported_languages: Set of supported programming languages
        supported_technologies: Set of supported technologies
        config: Validator configuration options
        validation_metrics: Metrics collected during validation
    """
    
    def __init__(
        self, 
        supported_languages: Optional[Set[str]] = None,
        supported_technologies: Optional[Set[str]] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """Initialize the example validator.
        
        Args:
            supported_languages: Set of supported programming languages
            supported_technologies: Set of supported technologies
            config: Validator configuration options
        """
        self.logger = logging.getLogger(__name__)
        self.supported_languages = supported_languages or set()
        self.supported_technologies = supported_technologies or set()
        self.config = config or {}
        self.validation_metrics = self._reset_metrics()
        
        # Load language-specific validation rules if available
        self.language_rules = self._load_language_rules()
        
        # Initialize technology mapping from configuration
        self.technology_mapping = self.config.get('technology_mapping', {})
        
        self.logger.info("ExampleValidator initialized with %d supported languages and %d supported technologies",
                        len(self.supported_languages), len(self.supported_technologies))
    
    def _load_language_rules(self) -> Dict[str, Dict[str, Any]]:
        """Load language-specific validation rules.
        
        Returns:
            Dict[str, Dict[str, Any]]: Dictionary of language validation rules
        """
        rules_file = self.config.get('language_rules_file')
        rules = {}
        
        if not rules_file:
            # Default built-in rules
            rules = {
                'python': {
                    'required_tokens': [],
                    'forbidden_tokens': ['exec(', 'eval(', 'os.system('],
                    'check_indentation': True,
                    'check_syntax': True,
                    'check_docstrings': True
                },
                'javascript': {
                    'required_tokens': [],
                    'forbidden_tokens': ['eval(', 'new Function(', 'document.write('],
                    'check_syntax': True,
                    'check_semicolons': True
                },
                'java': {
                    'required_tokens': ['public class'],
                    'forbidden_tokens': ['System.exit('],
                    'check_syntax': True,
                    'check_braces': True
                }
            }
        else:
            # Load rules from configuration file
            try:
                with open(rules_file, 'r') as f:
                    rules = json.load(f)
                self.logger.info("Loaded language rules from %s", rules_file)
            except Exception as e:
                self.logger.warning("Failed to load language rules from %s: %s", rules_file, e)
                # Fall back to default rules
                pass
                
        return rules
    
    def _reset_metrics(self) -> Dict[str, Any]:
        """Reset validation metrics to initial state.
        
        Returns:
            Dict[str, Any]: Empty metrics structure
        """
        return {
            "total": 0,
            "valid": 0,
            "invalid": 0,
            "validation_time": 0,
            "validation_rate": 0,
            "valid_percentage": 0,
            "error_types": {},
            "by_query_type": {},
            "by_difficulty": {},
            "by_domain": {},
            "by_language": {},
            "code_validation": {
                "attempted": 0,
                "successful": 0,
                "failed": 0,
                "success_rate": 0,
                "by_language": {}
            },
            "quality_issues": defaultdict(int),
            "warnings": defaultdict(int)
        }
    
    def validate_examples(self, examples: List[Example]) -> Tuple[List[Example], Dict[str, Any]]:
        """Validate the generated examples and provide detailed validation metrics.
        
        Performs comprehensive validation of examples, including code snippet
        validation, consistency checks, and quality assessment. Collects detailed
        metrics about the validation process and results.
        
        Args:
            examples: List of examples to validate
            
        Returns:
            Tuple[List[Example], Dict[str, Any]]: Valid examples and validation metrics
        """
        if not examples:
            self.logger.warning("No examples to validate")
            return [], self._reset_metrics()
            
        self.logger.info("Validating %d examples", len(examples))
        valid_examples = []
        
        # Reset validation metrics
        self.validation_metrics = self._reset_metrics()
        self.validation_metrics["total"] = len(examples)
        
        # Start timing validation process
        validation_start = time.time()
        
        # Initialize metrics by query type and difficulty level
        self._initialize_metrics(examples)
        
        # Validate each example
        for example in examples:
            # Perform detailed validation
            validation_result = self._validate_example_with_details(example)
            
            # Update metrics based on validation result
            self._update_metrics(example, validation_result)
            
            # Add to valid examples if valid
            if validation_result["is_valid"]:
                valid_examples.append(example)
        
        # Finalize metrics
        self._finalize_metrics(validation_start)
        
        # Log validation summary
        self.logger.info(
            "Validation complete: %d/%d examples valid (%.1f%%) in %.2f seconds",
            self.validation_metrics["valid"],
            self.validation_metrics["total"],
            self.validation_metrics["valid_percentage"],
            self.validation_metrics["validation_time"]
        )
        
        return valid_examples, self.validation_metrics
    
    def _initialize_metrics(self, examples: List[Example]) -> None:
        """Initialize detailed metrics by query type and difficulty.
        
        Args:
            examples: List of examples to initialize metrics for
        """
        # Initialize metrics for each query type and difficulty level
        for example in examples:
            query_type = example.query_info.query_type.value
            difficulty = example.query_info.difficulty.value
            domain = example.query_info.components.domain or "unknown"
            language = example.query_info.components.language or "unknown"
            
            # Initialize query type metrics if not already present
            if query_type not in self.validation_metrics["by_query_type"]:
                self.validation_metrics["by_query_type"][query_type] = {
                    "total": 0,
                    "valid": 0,
                    "invalid": 0
                }
            
            # Initialize difficulty metrics if not already present
            if difficulty not in self.validation_metrics["by_difficulty"]:
                self.validation_metrics["by_difficulty"][difficulty] = {
                    "total": 0,
                    "valid": 0,
                    "invalid": 0
                }
            
            # Initialize domain metrics if not already present
            if domain not in self.validation_metrics["by_domain"]:
                self.validation_metrics["by_domain"][domain] = {
                    "total": 0,
                    "valid": 0,
                    "invalid": 0
                }
            
            # Initialize language metrics if not already present
            if language not in self.validation_metrics["by_language"]:
                self.validation_metrics["by_language"][language] = {
                    "total": 0,
                    "valid": 0,
                    "invalid": 0
                }
            
            # Increment total counts
            self.validation_metrics["by_query_type"][query_type]["total"] += 1
            self.validation_metrics["by_difficulty"][difficulty]["total"] += 1
            self.validation_metrics["by_domain"][domain]["total"] += 1
            self.validation_metrics["by_language"][language]["total"] += 1
    
    def _update_metrics(self, example: Example, result: Dict[str, Any]) -> None:
        """Update validation metrics based on validation result.
        
        Args:
            example: Validated example
            result: Validation result with details
        """
        query_type = example.query_info.query_type.value
        difficulty = example.query_info.difficulty.value
        domain = example.query_info.components.domain or "unknown"
        language = example.query_info.components.language or "unknown"
        
        # Update metrics based on validation result
        if result["is_valid"]:
            self.validation_metrics["valid"] += 1
            self.validation_metrics["by_query_type"][query_type]["valid"] += 1
            self.validation_metrics["by_difficulty"][difficulty]["valid"] += 1
            self.validation_metrics["by_domain"][domain]["valid"] += 1
            self.validation_metrics["by_language"][language]["valid"] += 1
        else:
            self.validation_metrics["invalid"] += 1
            self.validation_metrics["by_query_type"][query_type]["invalid"] += 1
            self.validation_metrics["by_difficulty"][difficulty]["invalid"] += 1
            self.validation_metrics["by_domain"][domain]["invalid"] += 1
            self.validation_metrics["by_language"][language]["invalid"] += 1
            
            # Track error type
            error_type = result["error_type"]
            self.validation_metrics["error_types"][error_type] = self.validation_metrics["error_types"].get(error_type, 0) + 1
            
            # Log error details
            self.logger.warning(
                "Example %s failed validation: %s - %s",
                example.id,
                error_type,
                result["error_message"]
            )
        
        # Track warnings
        for warning in result.get("warnings", []):
            self.validation_metrics["warnings"][warning] += 1
    
    def _finalize_metrics(self, start_time: float) -> None:
        """Calculate final validation metrics.
        
        Args:
            start_time: Validation start time
        """
        # Calculate validation time
        self.validation_metrics["validation_time"] = time.time() - start_time
        
        # Calculate validation rate
        total = self.validation_metrics["total"]
        validation_time = self.validation_metrics["validation_time"]
        self.validation_metrics["validation_rate"] = total / validation_time if validation_time > 0 else 0
        
        # Calculate valid percentage
        self.validation_metrics["valid_percentage"] = (
            self.validation_metrics["valid"] / total * 100 if total > 0 else 0
        )
        
        # Calculate code validation success rate
        code = self.validation_metrics["code_validation"]
        code["success_rate"] = code["successful"] / code["attempted"] if code["attempted"] > 0 else 0
        
        # Calculate rates for each query type and difficulty
        for category in ["by_query_type", "by_difficulty", "by_domain", "by_language"]:
            for key, stats in self.validation_metrics[category].items():
                if stats["total"] > 0:
                    stats["valid_percentage"] = (stats["valid"] / stats["total"]) * 100
                else:
                    stats["valid_percentage"] = 0
    
    def _validate_example_with_details(self, example: Example) -> Dict[str, Any]:
        """Validate an example with detailed error information.
        
        Performs comprehensive validation of an example, including structural
        validation, code validation, technology compatibility, query text
        consistency, and content quality assessment.
        
        Args:
            example: Example to validate
            
        Returns:
            Dict[str, Any]: Validation result with details
        """
        # Initialize validation result
        validation_result = {
            "is_valid": True,
            "error_type": None,
            "error_message": None,
            "warnings": []
        }
        
        # Validate basic structure
        if not example.query_info.query:
            validation_result.update(
                is_valid=False,
                error_type="EmptyQuery",
                error_message="Example has empty query"
            )
            return validation_result
        
        if not example.internal_reasoning:
            validation_result.update(
                is_valid=False,
                error_type="EmptyReasoning",
                error_message="Example has empty internal reasoning"
            )
            return validation_result
        
        if not example.external_response:
            validation_result.update(
                is_valid=False,
                error_type="EmptyResponse",
                error_message="Example has empty external response"
            )
            return validation_result
        
        # Validate code examples if enabled
        if self.config.get('validate_code_examples', True):
            comp = example.query_info.components
            
            # Validate primary code snippet
            if comp.code_snippet and comp.language:
                self.validation_metrics["code_validation"]["attempted"] += 1
                code_result = self._validate_code_snippet(
                    comp.code_snippet,
                    comp.language,
                    is_debug_snippet=False
                )
                
                if not code_result["is_valid"]:
                    validation_result.update(
                        is_valid=False,
                        error_type="InvalidCodeSnippet",
                        error_message=code_result["error_message"]
                    )
                    return validation_result
                
                # Add warnings
                validation_result["warnings"].extend(code_result.get("warnings", []))
            
            # Validate debug code snippet if present
            if comp.debug_code_snippet and comp.language:
                self.validation_metrics["code_validation"]["attempted"] += 1
                code_result = self._validate_code_snippet(
                    comp.debug_code_snippet,
                    comp.language,
                    is_debug_snippet=True
                )
                
                if not code_result["is_valid"]:
                    validation_result.update(
                        is_valid=False,
                        error_type="InvalidDebugCodeSnippet",
                        error_message=code_result["error_message"]
                    )
                    return validation_result
                
                # Add warnings
                validation_result["warnings"].extend(code_result.get("warnings", []))
        
        # Validate technology-domain compatibility if enabled
        if self.config.get('validate_technology_mapping', True):
            comp = example.query_info.components
            if comp.domain and comp.technology:
                is_valid_tech = self._validate_technology(comp.technology, comp.domain)
                if not is_valid_tech:
                    validation_result.update(
                        is_valid=False,
                        error_type="InvalidTechnologyMapping",
                        error_message=f"Technology '{comp.technology}' not valid for domain '{comp.domain}'"
                    )
                    return validation_result
        
        # Validate query text consistency
        if not self._validate_query_text_consistency(example.query_info):
            validation_result.update(
                is_valid=False,
                error_type="InconsistentQueryText",
                error_message="Query text does not match query components"
            )
            return validation_result
        
        # Validate framework-specific content
        framework_issues = self._validate_framework_content(example)
        if framework_issues:
            validation_result["warnings"].extend(framework_issues)
        
        # Validate content quality
        quality_issues = self._validate_content_quality(example)
        if quality_issues:
            for issue in quality_issues:
                self.validation_metrics["quality_issues"][issue] += 1
                
            # Add warnings
            validation_result["warnings"].extend(quality_issues)
            
            # Fail validation if severe quality issues are configured to trigger failure
            if self.config.get('fail_on_quality_issues', False):
                validation_result.update(
                    is_valid=False,
                    error_type="QualityIssue",
                    error_message=quality_issues[0]
                )
                return validation_result
        
        return validation_result
    
    def _validate_code_snippet(
        self, 
        code_snippet: str, 
        language: str,
        is_debug_snippet: bool = False
    ) -> Dict[str, Any]:
        """Validate a code snippet for correctness and quality.
        
        Performs syntax checking, structure validation, and quality checks
        for code snippets. Debug snippets are allowed to have deliberate
        errors if they are part of the debugging scenario.
        
        Args:
            code_snippet: Code snippet to validate
            language: Programming language of the snippet
            is_debug_snippet: Whether this is a debug snippet that may contain deliberate errors
            
        Returns:
            Dict[str, Any]: Validation result with details
        """
        # Initialize validation result
        validation_result = {
            "is_valid": True,
            "error_message": None,
            "warnings": [],
            "metrics": {}
        }
        
        # Normalize language name
        language_lower = language.lower().strip()
        language_aliases = {
            "py": "python",
            "js": "javascript",
            "ts": "typescript",
            "csharp": "c#",
            "c++": "cpp",
            "golang": "go"
        }
        language_lower = language_aliases.get(language_lower, language_lower)
        
        # Track metrics by language
        if language_lower not in self.validation_metrics["code_validation"]["by_language"]:
            self.validation_metrics["code_validation"]["by_language"][language_lower] = {
                "attempted": 0,
                "successful": 0,
                "failed": 0
            }
        
        lang_metrics = self.validation_metrics["code_validation"]["by_language"][language_lower]
        lang_metrics["attempted"] += 1
        
        # Basic validation
        if not code_snippet or len(code_snippet.strip()) < 5:
            validation_result.update(
                is_valid=False,
                error_message="Code snippet is empty or too short"
            )
            lang_metrics["failed"] += 1
            self.validation_metrics["code_validation"]["failed"] += 1
            return validation_result
        
        # Accept placeholders (for example templates)
        if "Placeholder for" in code_snippet and "code" in code_snippet:
            lang_metrics["successful"] += 1
            self.validation_metrics["code_validation"]["successful"] += 1
            validation_result["warnings"].append("Contains placeholder code - not fully validated")
            return validation_result
        
        # Load language-specific rules
        lang_rules = self.language_rules.get(language_lower, {})
        
        try:
            # Python-specific validation
            if language_lower == "python":
                self._validate_python_code(code_snippet, validation_result, is_debug_snippet, lang_rules)
                
            # JavaScript-specific validation
            elif language_lower in ["javascript", "js"]:
                self._validate_javascript_code(code_snippet, validation_result, is_debug_snippet, lang_rules)
                
            # Java-specific validation
            elif language_lower == "java":
                self._validate_java_code(code_snippet, validation_result, is_debug_snippet, lang_rules)
                
            # Check for required tokens if specified
            required_tokens = lang_rules.get("required_tokens", [])
            for token in required_tokens:
                if token not in code_snippet:
                    validation_result["warnings"].append(f"Missing required token: {token}")
            
            # Check for forbidden tokens if specified
            forbidden_tokens = lang_rules.get("forbidden_tokens", [])
            for token in forbidden_tokens:
                if token in code_snippet:
                    validation_result["warnings"].append(f"Contains forbidden token: {token}")
                    
                    # Fail validation for security issues if configured
                    if token in self.config.get('security_critical_tokens', []):
                        validation_result.update(
                            is_valid=False,
                            error_message=f"Contains security-critical forbidden token: {token}"
                        )
                        lang_metrics["failed"] += 1
                        self.validation_metrics["code_validation"]["failed"] += 1
                        return validation_result
            
            # Check for common coding issues in all languages
            self._validate_common_code_issues(code_snippet, validation_result)
            
            # Mark as successful
            lang_metrics["successful"] += 1
            self.validation_metrics["code_validation"]["successful"] += 1
            
        except Exception as e:
            # Log but don't fail validation on validation errors themselves
            self.logger.warning(f"Error during code validation for {language}: {e}")
            validation_result["warnings"].append(f"Validation error: {str(e)}")
            lang_metrics["successful"] += 1  # Count as successful to avoid penalizing
            self.validation_metrics["code_validation"]["successful"] += 1
        
        return validation_result
    
    def _validate_python_code(
        self, 
        code_snippet: str, 
        validation_result: Dict[str, Any],
        is_debug_snippet: bool,
        lang_rules: Dict[str, Any]
    ) -> None:
        """Validate Python code for syntax and quality issues.
        
        Args:
            code_snippet: Python code snippet
            validation_result: Validation result to update
            is_debug_snippet: Whether this is a debug snippet
            lang_rules: Language-specific validation rules
        """
        # Skip syntax checking for debug snippets if configured
        check_syntax = lang_rules.get("check_syntax", True)
        if check_syntax and not is_debug_snippet:
            try:
                # Parse the code to check for syntax errors
                tree = ast.parse(code_snippet)
                
                # Basic check for undefined variables
                if lang_rules.get("check_undefined_vars", True):
                    defined = {node.id for node in ast.walk(tree) 
                              if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store)}
                    used = {node.id for node in ast.walk(tree) 
                           if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load)}
                    
                    # Get builtins and common imports to avoid false positives
                    builtins = set(dir(__builtins__))
                    common_imports = {"os", "sys", "re", "json", "math", "time", "datetime", 
                                     "random", "numpy", "np", "pandas", "pd", "matplotlib", 
                                     "plt", "torch", "tf", "sklearn"}
                    
                    undefined = used - defined - builtins - common_imports
                    if undefined:
                        validation_result["warnings"].append(f"Potential undefined variables: {', '.join(undefined)}")
                
                # Check for unused imports
                if lang_rules.get("check_unused_imports", True):
                    imports = set()
                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            for name in node.names:
                                imports.add(name.name)
                        elif isinstance(node, ast.ImportFrom):
                            module = node.module
                            for name in node.names:
                                if name.name != '*':
                                    imports.add(f"{module}.{name.name}" if module else name.name)
                    
                    # Check if imports are used
                    used_names = {node.id for node in ast.walk(tree) 
                                 if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load)}
                    
                    used_attrs = {node.attr for node in ast.walk(tree) 
                                 if isinstance(node, ast.Attribute) and isinstance(node.ctx, ast.Load)}
                    
                    unused_imports = []
                    for imp in imports:
                        parts = imp.split('.')
                        if not any(part in used_names or part in used_attrs for part in parts):
                            unused_imports.append(imp)
                    
                    if unused_imports:
                        validation_result["warnings"].append(f"Unused imports: {', '.join(unused_imports)}")
                
            except SyntaxError as e:
                # Don't fail debug snippets on syntax errors
                if not is_debug_snippet:
                    validation_result.update(
                        is_valid=False,
                        error_message=f"Python syntax error: {e}"
                    )
                    return
        
        # Check for docstrings if required
        if lang_rules.get("check_docstrings", False):
            has_class_or_func = "class " in code_snippet or "def " in code_snippet
            has_docstring = '"""' in code_snippet or "'''" in code_snippet
            
            if has_class_or_func and not has_docstring:
                validation_result["warnings"].append("Functions or classes without docstrings")
        
        # Check for indentation issues
        if lang_rules.get("check_indentation", False):
            lines = code_snippet.split('\n')
            for i, line in enumerate(lines):
                if line.strip() and line[0] == ' ' and len(line) - len(line.lstrip(' ')) % 4 != 0:
                    validation_result["warnings"].append(f"Inconsistent indentation at line {i+1}")
    
    def _validate_javascript_code(
        self, 
        code_snippet: str, 
        validation_result: Dict[str, Any],
        is_debug_snippet: bool,
        lang_rules: Dict[str, Any]
    ) -> None:
        """Validate JavaScript code for syntax and quality issues.
        
        Args:
            code_snippet: JavaScript code snippet
            validation_result: Validation result to update
            is_debug_snippet: Whether this is a debug snippet
            lang_rules: Language-specific validation rules
        """
        # Basic syntax checks
        check_syntax = lang_rules.get("check_syntax", True)
        if check_syntax and not is_debug_snippet:
            # Check for unmatched brackets
            brackets = {'(': ')', '{': '}', '[': ']'}
            stack = []
            
            for char in code_snippet:
                if char in brackets.keys():
                    stack.append(char)
                elif char in brackets.values():
                    if not stack or brackets[stack.pop()] != char:
                        validation_result.update(
                            is_valid=False,
                            error_message=f"Unmatched bracket: {char}"
                        )
                        return
            
            if stack:
                validation_result.update(
                    is_valid=False,
                    error_message=f"Unclosed brackets: {', '.join(stack)}"
                )
                return
        
        # Check for consistent semicolon usage
        if lang_rules.get("check_semicolons", False):
            lines = code_snippet.split('\n')
            statements_without_semicolons = 0
            total_statements = 0
            
            for line in lines:
                line = line.strip()
                # Skip comments, empty lines, and lines that shouldn't end with semicolons
                if (not line or line.startswith('//') or line.startswith('/*') or 
                        line.endswith('{') or line.endswith('}') or
                        line.startswith('import') or line.endswith(',')):
                    continue
                
                total_statements += 1
                if not line.endswith(';'):
                    statements_without_semicolons += 1
            
            # Warn if inconsistent semicolon usage (either mostly used or mostly not used)
            if total_statements > 0:
                semicolon_ratio = statements_without_semicolons / total_statements
                if 0.1 < semicolon_ratio < 0.9:
                    validation_result["warnings"].append("Inconsistent semicolon usage")
        
        # Check for consistent quote style
        if lang_rules.get("check_quotes", False):
            single_quotes = code_snippet.count("'")
            double_quotes = code_snippet.count('"')
            backticks = code_snippet.count('`')
            
            # Warn if mixing quote styles (excluding template literals)
            if single_quotes > 0 and double_quotes > 0 and backticks == 0:
                validation_result["warnings"].append("Mixed quote styles (single and double quotes)")
        
        # Check for console.log statements
        if lang_rules.get("check_console_logs", False) and "console.log" in code_snippet:
            validation_result["warnings"].append("Contains console.log statements")
    
    def _validate_java_code(
        self, 
        code_snippet: str, 
        validation_result: Dict[str, Any],
        is_debug_snippet: bool,
        lang_rules: Dict[str, Any]
    ) -> None:
        """Validate Java code for syntax and quality issues.
        
        Args:
            code_snippet: Java code snippet
            validation_result: Validation result to update
            is_debug_snippet: Whether this is a debug snippet
            lang_rules: Language-specific validation rules
        """
        # Check for class definition
        if lang_rules.get("check_class_definition", True) and not is_debug_snippet:
            if "class " not in code_snippet:
                validation_result["warnings"].append("No class definition found in Java code")
        
        # Check for proper braces
        if lang_rules.get("check_braces", True) and not is_debug_snippet:
            if code_snippet.count('{') != code_snippet.count('}'):
                validation_result.update(
                    is_valid=False,
                    error_message="Unmatched braces in Java code"
                )
                return
        
        # Check for main method if it's a full program
        if lang_rules.get("check_main_method", False) and "class " in code_snippet:
            # Check if this is likely a complete program (not just a class or method)
            if len(code_snippet.split('\n')) > 10 and "public static void main" not in code_snippet:
                validation_result["warnings"].append("Java program without main method")
        
        # Check for proper package declaration
        if lang_rules.get("check_package", False) and "class " in code_snippet:
            if not re.search(r'package\s+[a-z][a-z0-9_]*(\.[a-z][a-z0-9_]*)*\s*;', code_snippet):
                validation_result["warnings"].append("Missing or invalid package declaration")
        
        # Check for proper imports
        if lang_rules.get("check_imports", False):
            imports = re.findall(r'import\s+([^;]+);', code_snippet)
            for imp in imports:
                if not re.match(r'([a-z][a-z0-9_]*\.)+(\*|[A-Z][A-Za-z0-9_]*)', imp.strip()):
                    validation_result["warnings"].append(f"Invalid import statement: {imp}")
    
    def _validate_common_code_issues(
        self, 
        code_snippet: str, 
        validation_result: Dict[str, Any]
    ) -> None:
        """Validate common code issues across languages.
        
        Args:
            code_snippet: Code snippet to validate
            validation_result: Validation result to update
        """
        # Check for TODO comments
        if '// TODO' in code_snippet or '# TODO' in code_snippet or '/* TODO' in code_snippet:
            validation_result["warnings"].append("TODO comments found in code")
        
        # Check for potential hardcoded credentials
        credential_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'apikey\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']',
            r'token\s*=\s*["\'][^"\']+["\']'
        ]
        
        for pattern in credential_patterns:
            if re.search(pattern, code_snippet, re.IGNORECASE):
                validation_result["warnings"].append("Possible hardcoded credentials in code")
                break
        
        # Check for overly long lines
        lines = code_snippet.split('\n')
        long_lines = [i + 1 for i, line in enumerate(lines) if len(line) > 100]
        if long_lines:
            if len(long_lines) <= 3:
                validation_result["warnings"].append(f"Long lines (>100 chars) at lines: {', '.join(map(str, long_lines))}")
            else:
                validation_result["warnings"].append(f"Contains {len(long_lines)} long lines (>100 chars)")
        
        # Check for inconsistent line endings
        if '\r\n' in code_snippet and '\n' in code_snippet.replace('\r\n', ''):
            validation_result["warnings"].append("Mixed line endings (CRLF and LF)")
        
        # Check for commented-out code (simple heuristic)
        comment_lines = 0
        code_after_comment = False
        for line in lines:
            stripped = line.strip()
            if stripped.startswith('//') or stripped.startswith('#'):
                if any(keyword in stripped for keyword in ['if ', 'for ', 'while ', 'class ', 'function ', 'def ']):
                    comment_lines += 1
            elif comment_lines > 0:
                code_after_comment = True
                
        if comment_lines > 3 and code_after_comment:
            validation_result["warnings"].append(f"Contains {comment_lines} lines of commented-out code")
    
    def _validate_technology(self, technology: str, domain: str) -> bool:
        """Validate that a technology is valid for a domain.
        
        Args:
            technology: Technology to validate
            domain: Domain to validate against
            
        Returns:
            bool: True if the technology is valid for the domain, False otherwise
        """
        # If no supported technologies are provided, assume all are valid
        if not self.supported_technologies:
            return True
        
        # Normalize technology and domain
        tech_lower = technology.lower()
        domain_lower = domain.lower()
        
        # Check if technology is globally supported
        if tech_lower in self.supported_technologies:
            return True
        
        # Check domain-specific technology mapping
        domain_data = self.technology_mapping.get(domain_lower, {})
        valid_techs = set()
        
        # Extract valid technologies for this domain
        for tech in domain_data.get('technologies', []):
            if isinstance(tech, dict) and 'name' in tech:
                valid_techs.add(tech['name'].lower())
            elif isinstance(tech, str):
                valid_techs.add(tech.lower())
        
        # Check if technology is valid for this domain
        return tech_lower in valid_techs
    
    def _validate_query_text_consistency(self, query_info: QueryInfo) -> bool:
        """Validate that the query text is consistent with its components.
        
        Args:
            query_info: Query information to validate
            
        Returns:
            bool: True if the query text is consistent with its components
        """
        # Get query text and normalize
        query_text = query_info.query.lower()
        comp = query_info.components
        
        # Components that should be reflected in the query text
        key_components = []
        
        # Add components based on query type
        if query_info.query_type == QueryType.CONCEPT_EXPLANATION and comp.concept:
            key_components.append(comp.concept)
            
        if query_info.query_type == QueryType.PROBLEM_SOLVING and comp.problem_area:
            key_components.append(comp.problem_area)
            
        if comp.technology:
            key_components.append(comp.technology)
            
        if comp.language:
            key_components.append(comp.language)
        
        # No key components to check
        if not key_components:
            return True
        
        # Check if key components are in query text
        missing_components = []
        for component in key_components:
            component_lower = component.lower()
            
            # Handle multi-word components
            words = component_lower.split()
            if len(words) > 1:
                # Check if all words are present
                if all(word in query_text for word in words):
                    continue
                # Check if the whole phrase is present
                if component_lower in query_text:
                    continue
                missing_components.append(component)
            else:
                # Single word component
                if component_lower not in query_text:
                    missing_components.append(component)
        
        # Allow some flexibility - consider valid if at least 2/3 of components are present
        if missing_components:
            missing_ratio = len(missing_components) / len(key_components)
            return missing_ratio <= 0.34  # At most 1/3 missing
            
        return True
    
    def _validate_framework_content(self, example: Example) -> List[str]:
        """Validate content based on the framework type.
        
        Checks that the reasoning and response follow the expected structure
        and content requirements for the specified framework.
        
        Args:
            example: Example to validate
            
        Returns:
            List[str]: List of framework-specific issues found
        """
        issues = []
        framework = example.query_info.framework
        query_type = example.query_info.query_type
        
        # Validate Developer Clarification Model
        if framework == FrameworkType.DEVELOPER_CLARIFICATION:
            # For concept explanations, check for definition
            if query_type == QueryType.CONCEPT_EXPLANATION:
                concept = example.query_info.components.concept
                if concept:
                    definition_patterns = [
                        r'is a',
                        r'refers to',
                        r'defined as',
                        r'means',
                        r'represents'
                    ]
                    
                    has_definition = any(re.search(f"{re.escape(concept)}\\s+{pattern}", 
                                                 example.external_response, re.IGNORECASE) 
                                       for pattern in definition_patterns)
                    
                    if not has_definition:
                        issues.append("Concept explanation does not contain clear definition")
            
            # Check for comprehensive explanation sections
            if not any(header in example.internal_reasoning for header in [
                "Concept Definition", "Technical Context", "Practical Application", 
                "Learning Path", "Advanced Topics"
            ]):
                issues.append("Developer Clarification framework missing expected sections")
        
        # Validate Chain of Thought framework
        elif framework == FrameworkType.CHAIN_OF_THOUGHT:
            # Check for progressive reasoning
            if query_type in [QueryType.PROBLEM_SOLVING, QueryType.DEBUGGING]:
                cot_elements = [
                    "Problem Analysis", "Approach", "Solution Strategy", 
                    "Implementation Steps", "Edge Cases", "Testing"
                ]
                
                if not any(element in example.internal_reasoning for element in cot_elements):
                    issues.append("Chain of Thought framework missing expected reasoning elements")
                
                # For debugging, check for root cause identification
                if query_type == QueryType.DEBUGGING and "root cause" not in example.internal_reasoning.lower():
                    issues.append("Debugging response missing root cause analysis")
        
        # Validate Hybrid Decision Making framework
        elif framework == FrameworkType.HYBRID_DECISION_MAKING:
            # Check for decision factors
            if query_type == QueryType.STRATEGIC_DECISIONS:
                decision_elements = [
                    "Options", "Criteria", "Analysis", "Recommendation", 
                    "Tradeoffs", "Implementation"
                ]
                
                if not any(element in example.internal_reasoning for element in decision_elements):
                    issues.append("Hybrid Decision Making framework missing expected decision elements")
                
                # Check for pros and cons
                if "pros" not in example.external_response.lower() or "cons" not in example.external_response.lower():
                    issues.append("Strategic decision response missing pros and cons analysis")
        
        return issues
    
    def _validate_content_quality(self, example: Example) -> List[str]:
        """Validate the content quality of an example.
        
        Checks for issues like insufficient detail, placeholder content,
        and inconsistencies between reasoning and response.
        
        Args:
            example: Example to validate
            
        Returns:
            List[str]: List of quality issues found, empty if none
        """
        issues = []
        
        # Check for placeholder content
        placeholder_terms = ["placeholder", "TODO", "[FILL IN]", "to be completed"]
        for term in placeholder_terms:
            if term.lower() in example.internal_reasoning.lower():
                issues.append(f"Internal reasoning contains placeholder content: '{term}'")
            
            if term.lower() in example.external_response.lower():
                issues.append(f"External response contains placeholder content: '{term}'")
        
        # Check for minimal content length
        min_reasoning_words = self.config.get('min_reasoning_words', 50)
        min_response_words = self.config.get('min_response_words', 50)
        
        reasoning_words = len(example.internal_reasoning.split())
        response_words = len(example.external_response.split())
        
        if reasoning_words < min_reasoning_words:
            issues.append(f"Internal reasoning is too short ({reasoning_words} words, minimum {min_reasoning_words})")
        
        if response_words < min_response_words:
            issues.append(f"External response is too short ({response_words} words, minimum {min_response_words})")
        
        # Check for query type-specific content
        if example.query_info.query_type == QueryType.IMPLEMENTATION_REQUESTS:
            # Implementation requests should contain code examples
            if not any(marker in example.external_response for marker in ["```", "function", "class", "def "]):
                issues.append("Implementation request response does not contain code examples")
        
        elif example.query_info.query_type == QueryType.DEBUGGING:
            # Debugging responses should identify issues and provide fixed code
            if "issue" not in example.external_response.lower() and "error" not in example.external_response.lower():
                issues.append("Debugging response does not identify issues")
            
            if "```" not in example.external_response and "fixed code" not in example.external_response.lower():
                issues.append("Debugging response does not provide fixed code")
        
        elif example.query_info.query_type == QueryType.STRATEGIC_DECISIONS:
            # Strategic decisions should provide clear recommendations
            recommendation_terms = ["recommend", "suggestion", "best option", "preferred approach"]
            has_recommendation = any(term in example.external_response.lower() for term in recommendation_terms)
            
            if not has_recommendation:
                issues.append("Strategic decision response does not provide clear recommendations")
        
        # Check for content consistency
        inconsistencies = self._check_content_consistency(example)
        issues.extend(inconsistencies)
        
        return issues
    
    def _check_content_consistency(self, example: Example) -> List[str]:
        """Check for consistency between reasoning and response content.
        
        Args:
            example: Example to validate
            
        Returns:
            List[str]: List of inconsistency issues
        """
        issues = []
        
        # Get reasoning and response texts
        reasoning = example.internal_reasoning.lower()
        response = example.external_response.lower()
        
        # Extract key terms from reasoning (simple approach - could be improved)
        reasoning_sentences = reasoning.split('.')
        key_terms = set()
        
        for sentence in reasoning_sentences:
            # Look for definitions or key points
            if " is " in sentence:
                parts = sentence.split(" is ")
                if len(parts[0].split()) <= 5:  # Simple heuristic for a term
                    key_terms.add(parts[0].strip())
        
        # Filter to significant terms (more than one word or technical term)
        significant_terms = [term for term in key_terms 
                           if len(term.split()) > 1 or any(char.isdigit() for char in term)]
        
        # Check if key terms from reasoning appear in response
        missing_terms = []
        for term in significant_terms:
            if term not in response:
                missing_terms.append(term)
        
        # Only flag if multiple significant terms are missing
        if len(missing_terms) >= 2 and len(missing_terms) / len(significant_terms) > 0.25:
            issues.append(f"External response missing key terms from reasoning: {', '.join(missing_terms[:3])}")
        
        # Check for contradictions between reasoning and response
        contradiction_markers = [
            ("advantages", "disadvantages"),
            ("pros", "cons"),
            ("benefits", "drawbacks"),
            ("should", "should not"),
            ("recommended", "not recommended")
        ]
        
        for pos, neg in contradiction_markers:
            if pos in reasoning and neg in reasoning:
                # Check if both exist in response
                if (pos in response and neg not in response) or (neg in response and pos not in response):
                    issues.append(f"External response inconsistent with reasoning regarding {pos}/{neg}")
        
        return issues
    
    def get_validation_metrics(self) -> Dict[str, Any]:
        """Get validation metrics.
        
        Returns:
            Dict[str, Any]: Validation metrics
        """
        return self.validation_metrics
    
    def get_validation_summary(self) -> str:
        """Get a human-readable summary of validation results.
        
        Returns:
            str: Validation summary
        """
        # Create summary strings
        metrics = self.validation_metrics
        summary_lines = [
            f"Validation Summary:",
            f"  Total examples: {metrics['total']}",
            f"  Valid examples: {metrics['valid']} ({metrics['valid_percentage']:.1f}%)",
            f"  Invalid examples: {metrics['invalid']}",
            f"  Validation time: {metrics['validation_time']:.2f} seconds ({metrics['validation_rate']:.1f} examples/sec)"
        ]
        
        # Add error type breakdown if there are errors
        if metrics.get('error_types'):
            summary_lines.append("\nError Types:")
            for error_type, count in sorted(metrics['error_types'].items(), key=lambda x: x[1], reverse=True):
                summary_lines.append(f"  {error_type}: {count}")
        
        # Add code validation metrics
        code_metrics = metrics.get('code_validation', {})
        if code_metrics.get('attempted', 0) > 0:
            summary_lines.append("\nCode Validation:")
            summary_lines.append(f"  Attempted: {code_metrics['attempted']}")
            summary_lines.append(f"  Successful: {code_metrics['successful']} ({code_metrics['success_rate'] * 100:.1f}%)")
            summary_lines.append(f"  Failed: {code_metrics['failed']}")
            
            # Add language breakdown
            if code_metrics.get('by_language'):
                summary_lines.append("\n  By Language:")
                for lang, lang_metrics in sorted(code_metrics['by_language'].items()):
                    if lang_metrics.get('attempted', 0) > 0:
                        success_rate = lang_metrics['successful'] / lang_metrics['attempted'] * 100
                        summary_lines.append(f"    {lang}: {lang_metrics['successful']}/{lang_metrics['attempted']} ({success_rate:.1f}%)")
        
        # Add quality issue metrics
        if metrics.get('quality_issues'):
            summary_lines.append("\nQuality Issues:")
            for issue, count in sorted(metrics['quality_issues'].items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    summary_lines.append(f"  {issue}: {count}")
        
        # Add warning metrics
        if metrics.get('warnings'):
            summary_lines.append("\nWarnings:")
            for warning, count in sorted(metrics['warnings'].items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    summary_lines.append(f"  {warning}: {count}")
        
        return "\n".join(summary_lines)
    
    def clear_metrics(self) -> None:
        """Clear validation metrics."""
        self.validation_metrics = self._reset_metrics()
    
    def batch_validate(
        self,
        examples: List[Example],
        batch_size: int = 100,
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> Tuple[List[Example], Dict[str, Any]]:
        """Validate examples in batches to prevent memory issues with large datasets.
        
        Args:
            examples: List of examples to validate
            batch_size: Number of examples to validate in each batch
            progress_callback: Optional callback function for progress updates
            
        Returns:
            Tuple[List[Example], Dict[str, Any]]: Valid examples and validation metrics
        """
        if not examples:
            return [], self._reset_metrics()
            
        total_examples = len(examples)
        self.logger.info("Batch validating %s examples with batch size %s", total_examples, batch_size)
        
        # Reset metrics
        self.validation_metrics = self._reset_metrics()
        self.validation_metrics["total"] = total_examples
        
        valid_examples = []
        processed = 0
        
        # Process in batches
        for i in range(0, total_examples, batch_size):
            batch = examples[i:i+batch_size]
            batch_valid, batch_metrics = self.validate_examples(batch)
            
            # Add valid examples
            valid_examples.extend(batch_valid)
            
            # Update progress
            processed += len(batch)
            if progress_callback:
                progress_callback(processed, total_examples)
                
            # Update metrics (excluding totals)
            self.validation_metrics["valid"] += batch_metrics["valid"]
            self.validation_metrics["invalid"] += batch_metrics["invalid"]
            self.validation_metrics["validation_time"] += batch_metrics["validation_time"]
            
            # Update error types
            for error_type, count in batch_metrics.get("error_types", {}).items():
                self.validation_metrics["error_types"][error_type] = (
                    self.validation_metrics["error_types"].get(error_type, 0) + count
                )
                
            # Update code validation metrics
            code_metrics = self.validation_metrics["code_validation"]
            batch_code = batch_metrics.get("code_validation", {})
            code_metrics["attempted"] += batch_code.get("attempted", 0)
            code_metrics["successful"] += batch_code.get("successful", 0)
            code_metrics["failed"] += batch_code.get("failed", 0)
            
            # Update language-specific metrics
            for lang, lang_metrics in batch_code.get("by_language", {}).items():
                if lang not in code_metrics["by_language"]:
                    code_metrics["by_language"][lang] = {
                        "attempted": 0,
                        "successful": 0,
                        "failed": 0
                    }
                code_metrics["by_language"][lang]["attempted"] += lang_metrics.get("attempted", 0)
                code_metrics["by_language"][lang]["successful"] += lang_metrics.get("successful", 0)
                code_metrics["by_language"][lang]["failed"] += lang_metrics.get("failed", 0)
                
            # Update category metrics
            for category in ["by_query_type", "by_difficulty", "by_domain", "by_language"]:
                for key, stats in batch_metrics.get(category, {}).items():
                    if key not in self.validation_metrics[category]:
                        self.validation_metrics[category][key] = {
                            "total": 0,
                            "valid": 0,
                            "invalid": 0
                        }
                    # Only update valid and invalid counts (total was set at initialization)
                    self.validation_metrics[category][key]["valid"] += stats.get("valid", 0)
                    self.validation_metrics[category][key]["invalid"] += stats.get("invalid", 0)
                    
            # Update quality issues
            for issue, count in batch_metrics.get("quality_issues", {}).items():
                self.validation_metrics["quality_issues"][issue] += count
                
            # Update warnings
            for warning, count in batch_metrics.get("warnings", {}).items():
                self.validation_metrics["warnings"][warning] += count
        
        # Calculate final metrics
        if self.validation_metrics["validation_time"] > 0:
            self.validation_metrics["validation_rate"] = total_examples / self.validation_metrics["validation_time"]
            
        if total_examples > 0:
            self.validation_metrics["valid_percentage"] = (
                self.validation_metrics["valid"] / total_examples * 100
            )
            
        # Calculate code validation success rate
        code = self.validation_metrics["code_validation"]
        code["success_rate"] = code["successful"] / code["attempted"] if code["attempted"] > 0 else 0
        
        # Calculate rates for each category
        for category in ["by_query_type", "by_difficulty", "by_domain", "by_language"]:
            for key, stats in self.validation_metrics[category].items():
                if stats["total"] > 0:
                    stats["valid_percentage"] = (stats["valid"] / stats["total"]) * 100
                else:
                    stats["valid_percentage"] = 0
                    
        # Log summary
        self.logger.info(
            f"Batch validation complete: {len(valid_examples)}/{total_examples} "
            f"examples valid ({self.validation_metrics['valid_percentage']:.1f}%) "
            f"in {self.validation_metrics['validation_time']:.2f} seconds"
        )
        
        return valid_examples, self.validation_metrics
    
    def validate_example(self, example: Example) -> Tuple[bool, Dict[str, Any]]:
        """Validate a single example and return detailed validation result.
        
        Args:
            example: Example to validate
            
        Returns:
            Tuple[bool, Dict[str, Any]]: Validation status and detailed result
        """
        # Validate example
        validation_result = self._validate_example_with_details(example)
        
        # Return validation status and result
        return validation_result["is_valid"], validation_result
    
    def export_validation_results(
        self,
        examples: List[Example],
        validation_results: List[Dict[str, Any]],
        output_file: str
    ) -> None:
        """Export validation results to a file.
        
        Args:
            examples: List of examples
            validation_results: List of validation results
            output_file: Output file path
        """
        if len(examples) != len(validation_results):
            raise ValueError("Number of examples and validation results must match")
            
        # Prepare data
        results = []
        for i, (example, result) in enumerate(zip(examples, validation_results)):
            results.append({
                "id": example.id,
                "query": example.query_info.query,
                "query_type": example.query_info.query_type.value,
                "difficulty": example.query_info.difficulty.value,
                "is_valid": result["is_valid"],
                "error_type": result.get("error_type"),
                "error_message": result.get("error_message"),
                "warnings": result.get("warnings", [])
            })
            
        # Write to file
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
            
        self.logger.info(f"Exported validation results to {output_file}")

    def _parse_example(self, example_dict: Dict[str, Any]) -> Example:
        """Parse a dictionary into an Example object.
        
        Args:
            example_dict: Dictionary representation of an example
            
        Returns:
            Example: Parsed example object
        """
        # Extract metadata
        metadata = example_dict.get('metadata', {})
        
        # Parse query type
        query_type_str = metadata.get('queryType', '')
        try:
            query_type = QueryType.from_str(query_type_str)
        except ValueError:
            self.logger.warning(f"Invalid query type: {query_type_str}")
            query_type = QueryType.CONCEPT_EXPLANATION
        
        # Parse difficulty level
        difficulty_str = metadata.get('difficulty', '')
        try:
            difficulty = DifficultyLevel.from_str(difficulty_str)
        except ValueError:
            self.logger.warning(f"Invalid difficulty level: {difficulty_str}")
            difficulty = DifficultyLevel.BASIC
        
        # Parse components
        components_dict = metadata.get('components', {})
        components = QueryComponents()
        for key, value in components_dict.items():
            if hasattr(components, key):
                setattr(components, key, value)
        
        # Parse framework
        framework_str = metadata.get('framework', '')
        try:
            if framework_str == "Developer Clarification Model":
                framework = FrameworkType.DEVELOPER_CLARIFICATION
            elif framework_str == "Chain of Thought (CoT) Framework":
                framework = FrameworkType.CHAIN_OF_THOUGHT
            elif framework_str == "Hybrid Decision-Making Framework":
                framework = FrameworkType.HYBRID_DECISION_MAKING
            else:
                framework = FrameworkType.DEVELOPER_CLARIFICATION
        except:
            framework = FrameworkType.DEVELOPER_CLARIFICATION
        
        # Create query info
        query_info = QueryInfo(
            query_type=query_type,
            difficulty=difficulty,
            components=components,
            framework=framework,
            query=example_dict.get('query', '')
        )
        
        # Create example
        return Example(
            id=example_dict.get('id', ''),
            query_info=query_info,
            internal_reasoning=example_dict.get('internal_reasoning', ''),
            external_response=example_dict.get('external_response', '')
        )
